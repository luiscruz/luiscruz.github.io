---
layout: post
author: LuÃ­s Cruz
title: "Everything you need to know about Green AI"

show_image: True
mermaid: False
equation: False
draft: True
invisible: False
summary: "These are just a few notes taken about the implications of the massive energy consumption that stems from AI. I don't have short-term plans to finish the article, but I'll leave the draft around."
---

AI is growing bigger. chat gpt showed the potential. people use it for this this and that. It is already a tool that users cannot go without.

<span class="first-letter">S</span>ince 2012, the amount of computing used for AI training has been **doubling every 3.4 months**[^amodey2018ai]. Massive datacenters have been able to accommodate this huge increase of computing requirements. However, there is a growing concern on the carbon-footprint of these datacenters. 
It comes as no surprise that the famous natural language processing model GPT-3 is estimated to require approximately 190,000 kWh to be trained[^gpt3]. This is equivalent to the energy consumption of 1,000 cars driving 1,000 Km. Moreover, the total footprint is much higher when we sum the multiple iterations that were required to derive the final model.


talk about all the uses thatr chat gpt can have. not only in the chat windows but also in tools.

But of course AI is also a powerful tool to save the planet. AI-based solutions
are being created to address numerous sustainability issues[^ai-farm] â€“ enabling decarbonisation of energy networks, [precision farming], circular economy, combating the loss of biodiversity, creating [precise climate models] of our planet, efficiently managing cooling systems (e.g., [in data centres][gao2014machine]), and so on.

Despite the enormous potential of AI in making our planet more sustainable, its carbon footprint is still a thorn in the side. The existing trend of creating the most precise models is now being questioned. Do we really need massive models that use massive datasets? What is the right trade-off between accuracy and energy efficiency? Where do we draw the line?

This is why a new research field is emerging: **Green AI** is the branch of AI that deals with the environmental sustainability of developing, consuming, and maintaining AI systems. Personally, this is a topic that is close to my heart and I am focusing most of my research around this topic.

It is important to make a distinction between Green AI and AI for sustainability. One very interesting application of AI is to make human activities more sustainable. For example, AI is being used to make the electricity power grid more efficient or "greener". Make no mistake, this is not **Green AI**. Branding it Green AI is comparable to label "green transportation" in a recycling business that relies on fossil-fuel trucks.


In this article, we are going to *discuss the different levels where the environmental sustainability of AI needs to be tackled*, *what the community of AI practitioners and researchers is currently doing about this* and what is the role of Software Engineering in all of this. 


Existing levels to target gren ai.
  - Hardware vs Software
  - Enterprise, tinyML/edgeAI

The importance of Software Engineering.
  - desiging large software ystems. AI is just the top of the iceberg.
  - all the code that is created around the model
  - AI practitioners are the ones who create AI. The energy efficiency needs to be targeted to them.
  - 
  
How to help.
  - reduce
  - reuse
  - recycle
  - model simplification
  - 


notes: here we talk about why we should care about Green AI and the different approaches/levels to target.

https://huggingface.co/spaces/huggingchat/chat-ui/discussions/328

## State-of-the-art

So far, only a few researchers were able to get their hands dirty. Most literature about Green AI consists in **position papers**. I.e., papers that call for the need of changing the research agenda and motivate the importance of focusing on studying green AI. The most famous papers are "[Energy and policy considerations for deep learning in NLP][strubell2019energy]" by Strubell et al. (2019), "[Green AI][schwartz2020green]" by Schwartz et al. (2020), and "[On the Dangers of Stochastic Parrots: Can Language Models Be Too Big? ðŸ¦œ][bender2021dangers]" Bender et al. (2021). The later got really famous because some of the authors were required by their employer to remove their names â€“ allegedly due to the critical view on how the tech sector is leading innovation in AI.

Previous work [Strubell et al. (2019)] showcases the importance of 1) reporting training time and sensitivity to hyper-parameters of AI models, 2) having equitable access to computation resources amongst academics, and 3) prioritising computationally efficient hardware and algorithms. This is particularly relevant because it leads us to the exact reason why only a few researchers have been able to get their hands dirty: AI is not democratic.

Doing research on Green AI requires having access to huge computational power. If we want to study how to make AI less energy greedy, we need to be able to reproduce the existing AI models. Almost every AI researcher can relate to this issue. In sum, all these **position papers** around Green AI reveal a great will to make a difference accompanied with a huge list of setbacks.

There is of course a lot to be done. And that's what makes Green AI an interesting topic.
Moreover, there is already prior work that did not exactly focus on Green AI, but can be easily . The challenge is to collect all this knowledge in a single place. The [Green AI - Annotated Bibliography] lists and summarises all the papers that are relevant to Green AI. It is a great source to keep yourself up-to-date with the related work â€“ collaborations are welcome via pull requests!

## Data-centric Green AI

There is a new movement/clichÃ© in AI that calls for less data. The *modus operandi* in AI has been to collect more and more data and feed it to highly complex training pipelines. The new data-centric movement calls for a shift in perspective, instead of focusing on improving the training strategy one should focus on improving the data. For example, it is better to have smaller datasets that represent the key properties we want the model to learn than having massive datasets full of noise and unwanted biases.

The same movement is now growing in Green AI: **data-centric Green AI**. It is no surprise that bigger datasets lead to more energy consumption when training a model.
A preliminary study has shown this in detail with well-known machine learning algorithms.

[strubell2019energy]: https://luiscruz.github.io/green-ai/publications/2019-11-strubell-energy.html
[schwartz2020green]: https://luiscruz.github.io/green-ai/publications/2020-12-schwartz-green.html
[bender2021dangers]: https://luiscruz.github.io/green-ai/publications/2021-03-bender-parrots.html






Squeezing the accuracy.
The new clichÃ© about AI - Shift to data-centric AI

The increasing numbers of carbon emissions are worrisome but should not stay in the way of breakthrough advancements in this field. It rather should be a concern that grows side by side, enabling a sustainable and responsible scientific progress.

Hence, Green AI.

What is Green AI.

One could argue that the widely benefits of AI in our society make up for its environmental costs and that, consequently, addressing Green AI will mostly slow it down. But, in fact, it is quite the opposite:
Green AI is quintessential to enable advancements in AI while, at the same time, reach climate neutrality and high environmental standards.

What is currently being done to improve green AI
  Sarah SOA papers
  Green Data center

Yet, all these strategies still assume carbon emissions. For example, data centres that rely on renewable energy sources still need a *marginal power plant* that runs on fossil fuels[^carbon-intensity]. Hence, reducing electricity needs is quintessential to reach carbon neutrality: we need energy-efficient AI technologies.


What should be done that has not yet been done.

Different layers where green ai has to be considered:

- data centres - carbon of power grid, location, time of the day, renewable source.
- model training 
- model tuning to a new version of dataset (hyper-parameter tuning)
- model 

If you are a green AI advocate what can you do to make a difference?

- not all problems require deep neural networks and deep learning architectures. Instead, choosing less compute-intensive AI might go a long way for our environment. Or maybe no AI at all. (cf. Deepika Sandeep)

Finally we should democratise Green AI. (AI is not democratic, the bar is quite high. Green AI makes the bar even higher. We have the moral responsibility to make Green AI inclusive and accessible to anyone (not only tech giants)

[^gpt3]: Training the super natural language model GPT-3 requires approximately 190,000 kWh: <https://www.theregister.com/2020/11/04/gpt3_carbon_footprint_estimate/>. Retrieved on December 8, 2021.

[^ai-farm]: AI is being used to reduce the need for harmful chemicals in farming activities: <https://www.freethink.com/articles/farming-robot>. Retrieved on December 8, 2021.

[^carbon-intensity]: More about the marginal carbon footprint in the Principles of Green Software Engineering: <https://principles.green/principles/carbon-intensity/>. Retrieved on December 8, 2021.
[^amodey2018ai]: The computation resources required to train state-of-the-art AI models has been doubling every 3.4 months: <https://openai.com/blog/ai-and-compute/>. Retrieved on December 8, 2021

```bibtex
@article{amodey2018ai,
	author = {Dario Amodei and Danny Hernandez},
	journal = {Open AI},
	title = {AI and Compute},
	url = {https://openai.com/blog/ai-and-compute/},
	year = {2018},
	Bdsk-Url-1 = {https://openai.com/blog/ai-and-compute/}}
```


[precision farming]: https://doi.org/10.1146/annurev-resource-100518-093929
[precise climate models]: https://doi.org/10.1038/s41558-021-00986-y
[gao2014machine]: https://research.google/pubs/pub42542/

[Green AI - Annotated Bibliography]: https://luiscruz.github.io/green-ai/
